---
title: "Applied Data Analysis"
subtitle: "Analyzing geospatial data in R"
author: "Sherrie Xie, Tomas Oles"
date: "2025"
output: 
  html_document:
    toc: true
    toc_float: 
        collapsed: false
        smooth_scroll: true
    depth: 4 
    theme: paper 
    highlight: tango
---

```{r global options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

## Spatial data overview

First, let's understand how different types of spatial data is handled in R, beginning with vector data (i.e. points, lines and polygons) and ending with a brief raster example. 

### Polygon data
We will begin by looking at polyogn data. Let's look at Philadelphia census tracts as an example. Census shapefiles are free to download on the [Census website](https://www.census.gov/geo/maps-data/data/tiger-line.html), but we will load a version that has already been converted to an sf object. 

```{r}
library(sf)

# Load philly tracts data
pt_sf <- readRDS("data/philadelphia_tracts.rds")

# Note philly.tracts is an sf ("simple feature") object of type "MULTIPOLYGON"
class(pt_sf) 

# sf objects can be handled like data frames using standard commands
str(pt_sf)   # view structure
head(pt_sf)  # view first several rows
dim(pt_sf)   # view dimensions
pt_sf[1,]    # select first row
head(pt_sf$NAMELSAD10)  # select column by name  
head(pt_sf[,7])         # select column by number

# We can extract the geometry of philly.tracts with the st_geometry function
pt_geo <- st_geometry(pt_sf)
pt_geo
pt_geo[[1]]       # perimeter coordinates for the first census tract of the sf
pt_sf[1,]  # i.e. Census Tract 94

pt_geo[[2]]        # perimeter coordinates for the second census tract of the sf
pt_sf[2,]  # i.e. Census Tract 95

# Plot the geometry of philly.tracts with the base plot function
plot(pt_geo)

# The base plot function has some aesthetic options we can use to tweak our plots
plot(pt_geo, col = "lemonchiffon2")
plot(pt_geo, lwd = 2, border = "red")





```

### Line data 

Next let's look at an example of line data: streets in Philadelphia with bicycle access. This data was sourced directly from the [Philadelphia Bike Network](https://www.opendataphilly.org/dataset/bike-network). 

```{r}
bn_sf <- st_read("data/Bike_Network/Bike_Network.shp")  # read shapefile as an sf object
class(bn_sf)  # bn.sf is an sf object, which is a subclass of data.frame

# Once again, let's access the spatial attributes of this sf object with the st_geometry command. 
bn_geo <- st_geometry(bn_sf)
bn_geo[[2]]  # line segment 2
bn_sf[2,]

# Let's plot the bike network data
plot(bn_geo)

```

### Point data
As an example of point data, we will work with crime incidents that occurred in Philadelphia in September 2018. The full publicly available crime incidents database for Philadelphia is maintained by the Philadelphia Police Department and is available on the [OpenDataPhilly](https://www.opendataphilly.org/dataset/crime-incidents) website.

```{r}
library(tidyverse)

crime_aux <- read_csv("data/crime_incidents.csv")

head(crime_aux)

crime_aux <- crime_aux[which(!is.na(crime_aux$lat) & !is.na(crime_aux$lng)),]

crime <- st_as_sf(crime_aux, coords = c("lng","lat"))

# The crime data is an sf object of type POINT and includes information on the date, time and offense type for each incident

class(crime)
head(crime)

plot(st_geometry(crime))

# Let's take a look at offense types and use dplyr to filter by offense_type...
table(crime$text_general_code)
homicide <- filter(crime, text_general_code == "Homicide - Criminal")
fraud <- filter(crime, text_general_code == "Fraud")
# Note subsets of an sf object are also sf objects
class(homicide)
class(fraud)

# Plotting homicide and fraud incidents with the base plot function
plot(st_geometry(homicide))
plot(st_geometry(fraud))


```
```{r warning=FALSE}
# Points by themselves are not very easy to understand. Let's layer them on top of the tract polygons with add = TRUE
plot(pt_geo)
plot(st_geometry(fraud), col = "blue", alpha = 0.1, add = TRUE)
plot(st_geometry(homicide), col = "red", add = TRUE)
legend("bottomright", legend = c("Fraud", "Homicide"), title = "Offense type:", col = c("blue", "red"), pch = 1, bty = "n")
```

### Raster data
So far, we have considered point, line, and polygon data, all of which fall under the umbrella of vector data types. Rasters are a distinct GIS data type that we will consider only briefly because they cannot be handled with `sf` methods. We will look at the `volcano` dataset, which gives topographic information for Maunga Whau (a volcano located in Auckland, New Zealand) on a 10m by 10m grid. Because it is a relatively small raster, we can handle `volcano` using base functions. Larger rasters should be handled using the _raster_ package.

```{r}
library(datasets)

# The volcano dataset is a 87x61 matrix
class(volcano)
str(volcano)
filled.contour(volcano, color = terrain.colors, asp = 1)
```

## NYS Lyme incidence data

We will use the example of New York State county-aggregated Lyme disease incidence for 2014-2016 to try our hand at spatial analysis. This data is publicly available and can be accessed at the [Health Data NY](https://health.data.ny.gov/Health/Community-Health-Lyme-Disease-Incidence-Rate-per-1/6sxr-cqij) website. Raw data can be downloaded in .csv format. If you're curious to see how this tabular data can be merged with a New York State county shapefile (available at [NYS GIS](https://gis.ny.gov/gisdata/inventories/details.cfm?DSID=927)), you can see how this is done in the 'prep_nys_lyme_data.R' script file located in the 'data' folder of our project directory. But for now, we'll start with this data that has already been merged and converted to an 'sf' object.

```{r}
library(sf)

# Load NYS Lyme incidence data
lyme <- readRDS("data/nys_lyme_data.rds")

# Let's take a look at some data attributes
class(lyme)
head(lyme)

# Note that the variable Lyme.Incidence.Rate gives the county-level Lyme disease incidence per 100,000 population

# Once again, we can plot the spatial attrbutes of this data
plot(st_geometry(lyme))

```

This data is an example of regional rate data. An easy way to map this data is using the 'tmap' library. Let's load 'tmap' and create an interactive map with just a few lines of code.

```{r}
library(tmap)

tmap_mode("view")  # set mode to interactive
tm_shape(lyme) +    # specify sf object with geographic attribute of interest
  tm_polygons("Lyme.Incidence.Rate")  # specify column with value of interest
```

We have a missing value in St. Lawrence county. Let's remove this row from the data so it doesn't throw an error later in our analysis.

```{r}
lyme <- lyme[!is.na(lyme$Lyme.Incidence.Rate),]

# Let's map again 
tmap_mode("view")  # set mode to interactive
tm_shape(lyme) +    # specify sf object with geographic attribute of interest
  tm_polygons("Lyme.Incidence.Rate")  # specify column with value of interest
```


## Global clustering (Moran's I)

This section was adapted from a tutorial created by Manuel Gimmond, which can be found on his [github page](https://mgimond.github.io/simple_moransI_example/). 

### Assess data distribution

Let's begin by looking at the distribution of our Lyme incidence rate data. The Moran's *I* statistic is not robust to extreme values or outliers so we will need to transform our data if it deviates greatly from a normal distribution.

```{r}
# Five-number summary 
summary(lyme$Lyme.Incidence.Rate)

# Histogram
hist(lyme$Lyme.Incidence.Rate)

# Boxplot
boxplot(lyme$Lyme.Incidence.Rate, horizontal = TRUE)

```

Our data is skewed strongly to the right with lots of outliers much greater than the mean. Let's see if a log transformation can make our data look more normal.

```{r}

# Create new variable that is the log-transformed incidence rate
lyme$log_lyme_incidence <- log(lyme$Lyme.Incidence.Rate)

# Histogram
hist(lyme$log_lyme_incidence)


# Boxplot
boxplot(lyme$log_lyme_incidence, horizontal = TRUE)

summary(lyme$log_lyme_incidence)
```

That's much better! We can see what our log-transformed values look like on a map.

```{r}

tm_shape(lyme) +    # specify sf object with geographic attribute of interest
  tm_polygons("log_lyme_incidence")  # specify column with value of interest

```

### Define neighboring polygons
Now we're ready to begin our analysis. The first step is to define "neighboring" polygons. Recall that neighbors can be defined based on contiguity or distance or as the *k* nearest neighbors to each polygon. We'll use a **queen**-case contiguity-based definition, where any contiguous polygon that shares at least one vertex will be considered a neighbor. We can store the neighbors of each one of our polygons by creating an 'nb' object using the 'poly2nb' function from the 'spdep' library.

```{r}
library(spdep)

# Create nb object from Lyme dataset
lyme_nb <- poly2nb(lyme, queen = T) # queen case
class(lyme_nb)
str(lyme_nb)

# View the neighbors of the first polygon
lyme_nb[[1]]
lyme$NAME[1]
lyme$NAME[c(11, 20, 42, 45, 46, 47)]
```
### Assign weights to neighbors

Next, we'll assign weights to each neighboring polygon. We'll use the simplest option ('style="W"), which assigns equal weight to each neighboring polygon. In other words, the weight applied to the neigbors of a polygon will equal 1/(no. of neighbors for that polygon).

```{r}
# Calculate weights from nb object, we'll specify style = "W" for equal weights
lyme_weights <- nb2listw(lyme_nb, style = "W")
class(lyme_weights)

# View the weight of the first polygon's neighbors
str(lyme_weights, max.level = 1) # view the structure of lw, we'll set max.level = 1 for easier viewing
lyme_weights$weights[[1]]  # The weights of the neighbors for the first polygon (Albany)
                 # Recall that Albany has 6 neighbors

lyme$NAME[2]     # Allegheny
lyme_nb[[2]]          # Allegheny has 4 neighbors
lyme_weights$weights[[2]]  # The weights of the neighbors for Allegheny

```
### Perform hypothesis testing 
Now we can calculate the Moran's *I* statistic and perform hypothesis testing using 'moran.test' (analytical calculation) and 'moran.mc' (via Monte Carlo simulations). These functions require that we specify the variable of interest and the list of neighbor weights for each polygon. The option 'alternative = "greater"' specifies testing for *positive* spatial autocorrelation, which is also the default for these functions. The 'moran.mc' function also requires that we specify the number of simulations with option 'nsim'.


```{r}
# Analytical test - quicker computation but sensitive to irregularly distributed polygons
moran.test(lyme$log_lyme_incidence, lyme_weights, alternative = "two.sided")

# Monte Carlo (MC) simulation is slower but the preferred  method to calculate an accurate p-value
MC <- moran.mc(lyme$log_lyme_incidence, lyme_weights, nsim = 999, alternative = "greater")
MC
```
Next, we’ll have R compute the average neighbor income value for each polygon. These values are often referred to as spatially lagged values.

```{r}
log_lyme_incidence.lag <- lag.listw(lyme_weights, lyme$log_lyme_incidence)
log_lyme_incidence.lag

plot(log_lyme_incidence.lag ~ lyme$log_lyme_incidence, pch=16, asp=1)
M1 <- lm(log_lyme_incidence.lag ~ lyme$log_lyme_incidence)
summary(M1)
abline(M1, col="blue")

coef(M1)[2]
```
We can see the results of the MC simulations graphically by passing the output of MC model to the 'plot' function.

```{r}
plot(MC)
```



## Local clustering (local Moran)

The local Moran statistic is an extension of the Moran's *I* for the analysis of *local* (rather than global) spatial autocorrelation. There are some steps in common with the global clustering analysis we performed previously (for example, we have to calculate neighbor weights again) but there are key differences, due to particularities of the 'rgeoda' library.


### Assign neighbor weights 
We will once again find queen-case contiguous weights, though in 'rgeoda' we do this with the 'queen_weights' function. Note instead of a list of weights like we saw previously with the 'nb2listw' function, 'queen_weights' outputs an 'rgeoda' 'Weight' object, which has some nice features.

```{r}
library(rgeoda)

# Find queen-case contiguous weights
lyme_gweights <- queen_weights(lyme)
class(lyme_gweights)

# str function allows us to see a nice summary of the weights object
str(lyme_gweights)

# See the neighbors of the first polygon (Albany)
get_neighbors(lyme_gweights, 1)
lyme$NAME[1]
lyme$NAME[get_neighbors(lyme_gweights, 1)]

# See the neighbors of the first polygon (Allegany)
get_neighbors(lyme_gweights, 2)
lyme$NAME[2]

lyme$NAME[get_neighbors(lyme_gweights, 2)]

# See the neighbor weights of the first and second polygons
get_neighbors_weights(lyme_gweights, 1)
get_neighbors_weights(lyme_gweights, 2)

```


### Calculate Local Moran statistic

Now you can use your 'geoda' 'Weights' to calculate the Local Moran statistic at each polygon.
```{r}
# We will coerce our data variable into a one-column data frame because this is the format required by the local_moran function
log_lyme_df <- as.data.frame(lyme$log_lyme_incidence)

# Now we can run the local_moran function
lyme_lisa <- local_moran(lyme_gweights, log_lyme_df)

# local_moran returns a LISA object
class(lyme_lisa)

# Let's take a closer look at this LISA object
lyme_lisa$lisa_vals  # View local Moran's I values for each polygon
lyme_lisa$p_vals     # View pseudo p-values

```

Finally, we can make a map of our results! 'rgeoda' includes some nifty functions ('map_colors', 'map_labels' and 'map_clusters' to help us with our mapping).

```{r}
map_colors <- lisa_colors(lyme_lisa)
map_labels <- lisa_labels(lyme_lisa)
map_clusters <- lisa_clusters(lyme_lisa)

plot(st_geometry(lyme), 
     col=sapply(map_clusters, function(x){return(map_colors[[x+1]])}), 
     border = "#333333", lwd=0.2)
legend('topright', legend = map_labels, fill = map_colors, border = "#eeeeee", cex = 0.7)

```

# Mapping GDP Fall Due to Trade Sanctions on Russia

The ongoing conflict between Russia and Ukraine has caused significant economic disruptions, particularly through sanctions and changes in trade patterns. One way to analyze these disruptions is through _Input-Output (I-O) analysis_, a method that models the interdependencies between sectors in an economy. By examining _output multipliers_, we can understand how changes in one sector's demand affect the entire economy.

## Input-Output Analysis and Output Multipliers

The core formula used in I-O analysis to compute total output and output multipliers is derived from the _Leontief inverse matrix_. The formula is as follows:

$$
\mathbf{X} = (\mathbf{I} - \mathbf{A})^{-1} \mathbf{F}
$$

Where:
- $\mathbf{X}$ is the _output vector_, representing total output for each sector.
- $\mathbf{I}$ is the _identity matrix_.
- $\mathbf{A}$ is the _direct requirements matrix_ (or technical coefficient matrix), representing the inputs required from other sectors to produce one unit of output.
- $(\mathbf{I} - \mathbf{A})^{-1}$ is the _Leontief inverse matrix_, capturing the total intersectoral dependencies.
- $\mathbf{F}$ is the _final demand vector_, representing demand for goods and services from external sources (e.g., households, exports).

### Direct Requirements Matrix $\mathbf{A}$

The elements of the _direct requirements matrix_ $\mathbf{A}$ are calculated as:

$$
A_{ij} = \frac{Z_{ij}}{X_j}
$$

Where:
- $Z_{ij}$ is the amount of output from sector $i$ used as input by sector $j$.
- $X_j$ is the total output of sector $j$.

### Leontief Inverse $(\mathbf{I} - \mathbf{A})^{-1}$

The Leontief inverse matrix accounts for both direct and indirect effects of changes in final demand. It shows how changes in one sector affect the entire economy due to intersectoral linkages.

### Output Multipliers

The _output multipliers_ for sector $j$ represent the total increase in output across all sectors from a unit increase in the final demand of sector $j$. These multipliers are computed by summing the column elements of the Leontief inverse matrix:

$$
\text{Output Multiplier for Sector } j = \sum_{i} (\mathbf{I} - \mathbf{A})^{-1}_{ij}
$$

This formula helps quantify how changes in demand for products or services in one sector lead to changes in the overall economic output, showing the ripple effect through the supply chain.

```{r setup, include=TRUE}
# Load necessary libraries
library(tidyverse)   # for data wrangling
library(tmaptools)   # for thematic mapping tools
library(tmap)        # for creating maps
library(ggplot2)     # for data visualization
library(leaflet)     # for interactive maps
library(sf)          # for handling spatial data
library(dplyr)       # for data manipulation
library(raster)      # for working with raster data
```

## Data Loading
Note: that the output-output multipliers has already been computed for all economies. 

```{r, warning=FALSE}

# Load the CSV file with header
GVC_EU <- read.csv("data/outputs_Russia.csv", header = TRUE, sep=",", blank.lines.skip = FALSE)

# Rename the first column to "CNTR_ID"
names(GVC_EU)[1] <- "CNTR_ID"
```

## Load Shapefile

```{r, warning=FALSE}

# Load the shapefile for countries
auxmap <- st_read("data/Countries_SHP1_3mil/CNTR_RG_03M_2016_3035.shp", 
                  stringsAsFactors = FALSE)

# Display the structure of the shapefile to understand its content
str(auxmap)
```
## Merge Data On Economic Impact with Shapefil

```{r, warning=FALSE}
# Perform an inner join between the shapefile data and GVC_EU data
aux_map_data <- inner_join(auxmap, GVC_EU)

```


## Visualizing GDP Fall Due to Sanctions
### Create a Quatile Map

```{r, warning=FALSE}
# Create a choropleth map based on GDP fall using quantile style
auxQmap1 <- tm_shape(aux_map_data) +
  tm_fill(col = "gGDPc",   
          style = "quantile", 
          n = 8, 
          legend.hist = TRUE, 
          palette = "YlOrRd", 
          title = "GDP Fall Due to Trade Sanctions on Russia", 
          alpha = 0.6) +
  tm_layout(legend.outside = TRUE)

# Set tmap mode to view (interactive map mode)
tmap_mode("view")

# Display the map with adjusted view
auxQmap1 + tm_view(set.view = c(7, 51, 4), text.size.variable = TRUE)

# Return to current mode
#tmap_mode(current.mode)

```



## Hierarchical Clustering Style

```{r eval=FALSE, warning=FALSE, include=TRUE}
# Create a choropleth map using hierarchical clustering style for a different view
auxQmap2 <- tm_shape(aux_map_data) +
  tm_fill(col = "gGDPc",   
          style = "hclust", 
          n = 8, 
          legend.hist = TRUE, 
          palette = "YlOrRd", 
          title = "GDP Fall Due to Trade Sanctions on Russia", 
          alpha = 0.6) +
  tm_layout(legend.outside = TRUE)

# Set tmap mode to view (interactive map mode)
tmap_mode("view")

# Display the map with hierarchical clustering
auxQmap2 + tm_view(set.view = c(7, 51, 4), text.size.variable = TRUE)
```


## Mapping Trade in Intermediate and Final Products
### Intermediate Products


```{r,eval=FALSE, warning=FALSE, include=TRUE}

# Create a map for trade in intermediate products and its impact on GDP
auxQmap3 <- tm_shape(aux_map_data) +
  tm_fill(col = "gGDPc_A",   
          style = "hclust", 
          n = 8, 
          legend.hist = TRUE, 
          palette = "YlOrRd", 
          title = "GDP Impact Due to Trade Sanctions in Intermediate Products", 
          alpha = 0.6) +
  tm_layout(legend.outside = TRUE)

# Display the map in interactive view
tmap_mode("view")
auxQmap3 + tm_view(set.view = c(7, 51, 4), text.size.variable = TRUE)

```



### Intermediate products



```{r eval=FALSE, warning=FALSE, include=TRUE}

# Create a map for trade in intermediate products and its impact on GDP
auxQmap3 <- tm_shape(aux_map_data) +
  tm_fill(col = "gGDPc_A",   
          style = "hclust", 
          n = 8, 
          legend.hist = TRUE, 
          palette = "YlOrRd", 
          title = "GDP Impact Due to Trade Sanctions in Intermediate Products", 
          alpha = 0.6) +
  tm_layout(legend.outside = TRUE)

# Display the map in interactive view
tmap_mode("view")
auxQmap3 + tm_view(set.view = c(7, 51, 4), text.size.variable = TRUE)


```



### Final Products



```{r, eval=FALSE, warning=FALSE, include=TRUE}

# Create a map for trade in final products and its impact on GDP
auxQmap4 <- tm_shape(aux_map_data) +
  tm_fill(col = "gGDPc_Y",   
          style = "hclust", 
          n = 8, 
          legend.hist = TRUE, 
          palette = "YlOrRd", 
          title = "GDP Impact Due to Trade Sanctions in Final Products", 
          alpha = 0.6) +
  tm_layout(legend.outside = TRUE)

# Display the map in interactive view
tmap_mode("view")
auxQmap4 + tm_view(set.view = c(7, 51, 4), text.size.variable = TRUE)


```

## Maps in ggplot 


```{r}
library(maps)
library(countrycode)
library(gapminder)

dat_map <- map_data("world")
ggplot(dat_map, aes(x = long, y = lat,
group = group)) +
geom_polygon(fill = "white", colour = "black")

#Plot the first shot
gdp_map <- full_join(x=dat_map,y=gapminder, by= c("region"="country"))
ggplot(gdp_map, aes(x = long, y = lat,
                                       group = group, fill = log10(gdpPercap))) +
  geom_polygon() +
  scale_fill_gradient(low = "red", high = "green")

#Too much countries are missing 
gapminder$ccode <- countrycode(gapminder$country,
origin = "country.name",
destination = "wb")
dat_map$ccode <- countrycode(dat_map$region,
origin = "country.name",
destination = "wb")
gdp_map_codes <- full_join(x=dat_map,y=gapminder, by="ccode")


#Plot again after merging 
ggplot(gdp_map_codes, aes(x = long, y = lat,
                                       group = group, fill = log10(gdpPercap))) +
  geom_polygon() +
  scale_fill_gradient(low = "red", high = "green")

```

# Dynamic graphs (examples)


```{r echo=TRUE, fig.height=15, fig.width=20, message=FALSE, warning=FALSE, fig.retina=4}
library(tidyverse)
library(gapminder)
library(echarts4r)
library(gganimate)
library(ggiraph)
library(widgetframe)
library(ggthemes)
library(plotly)
library(viridis)
library(DT)

# country codes in gapminder::country_codes

gapminder_codes <- gapminder::country_codes

# countries with info in gapminder::gapminder_unfiltered

gapminder <-gapminder::gapminder_unfiltered

# We join both datasets with inner_join to get a dataset with the info by country, continent and country-code

gapminder <- gapminder %>%
  inner_join(gapminder_codes, by= "country") %>%
  mutate(code = iso_alpha)

# A map of the world (Antarctica removed)

world <- map_data("world") %>%
  filter(region != "Antarctica")


gapminder_data <- gapminder %>%
  inner_join(maps::iso3166 %>%
               select(a3, mapname), by= c(code = "a3")) %>%
  mutate(mapname = str_remove(mapname, "\\(.*"))


datagpmnd <- gapminder %>%
  mutate(Name = recode_factor(country,
                              `Congo, Dem. Rep.`= "Dem. Rep. Congo",
                              `Congo, Rep.`= "Congo",
                              `Cote d'Ivoire`= "Côte d'Ivoire",
                              `Central African Republic`= "Central African Rep.",
                              `Yemen, Rep.`= "Yemen",
                              `Korea, Rep.`= "Korea",
                              `Korea, Dem. Rep.`= "Dem. Rep. Korea",
                              `Czech Republic`= "Czech Rep.",
                              `Slovak Republic`= "Slovakia",
                              `Dominican Republic`= "Dominican Rep.",
                              `Equatorial Guinea`= "Eq. Guinea"))


datagpmnd %>%
  group_by(year) %>%
  e_chart(Name, timeline = TRUE) %>%
  e_map(lifeExp) %>%
  e_visual_map(min= 30, max= 90,
               type = 'piecewise') %>%
  e_title("Life expectancy by country and year", left = "center") %>%
  e_tooltip(
    trigger = "item",
    formatter = e_tooltip_choro_formatter())
```