<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Applied Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Tomáš Oleš" />
    <meta name="date" content="2025-01-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Applied Data Analysis
]
.subtitle[
## Classification
]
.author[
### Tomáš Oleš
]
.institute[
### Department of Economic Policy
]
.date[
### 2025
]

---







## Lecture overview 


 - The classification vs. regression problem
 
 - Logistic regression
  + two classes 
  + more than two classes
  
- Discriminant analysis

- Logistic regression vs. Latent Discriminant Analysis 



---

## Classification 

Qualitative variables take values in an unordered set `\(C\)`, such as:
   $$ \text{eye color} \in {\{} \text{brown, blue, green} \} $$
  $$ \text{email} \in \{ \text{spam, ham} \} $$

Given a feature vector `\(X\)` and a qualitative response `\(Y \in \mathcal{C}\)`. The classification tasks is to build a function `\(C(X)\)` that takes as input the feature vector `\(X\)` and predicts its value for `\(Y; i.e. C(X) \in \mathcal{C}\)`

Often, we estimate the *probabilities* of `\(X\)` belonging to each category in `\(\mathcal{C}\)`

--

For example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classifiation fraudulent or not.

---


## Example: Credit Card Default
.pull-left[
&lt;img src="13_week_p_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;
]


.pull-right[
&lt;img src="13_week_p_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;
]
---
## Can we use Linear regression?

Suppose for the `Default` classification tasks that we code

$$
Y =
`\begin{cases}
0, &amp; \text{if No} \\
1, &amp; \text{if Yes.}
\end{cases}`
$$
Can we simply perform a linear regression of `\(Y\)` on `\(X\)` and classify as `Yes` if `\(\hat{Y} &gt; 0.5\)`?
--
  + In this case of a binary outcome, linear regression does a good job as a classifier, and is equivalent to linear discriminant analysis, which we discuss later.
  
  + Since in the population `\(E(Y | X = x) = \Pr(Y = 1 | X = x)\)`, we might think that regression is perfect for this task.
  
  + However, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate.

---

## Linear versus Logistic Regression

&lt;img src="13_week_p_files/figure-html/unnamed-chunk-4-1.png" width="720" /&gt;

The orange marks indicate the response `\(Y\)` , either 0 or 1. Linear regression does not estimate `\(Pr(Y = 1|X)\)` well. Logistic regression seems well suited to the task.

---
## Linear Regression continued

Now suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.

$$
Y =
`\begin{cases}
1, &amp; \text{if stroke;} \\
2, &amp; \text{if drug overdose;} \\
3, &amp; \text{if epileptic seizure.}
\end{cases}`
$$
This coding suggests an ordering, and in fact implies that the difference between **stroke** and **drug overdose** is the same as between **drug overdose** and **epileptic seizure**.

&lt;br&gt;
--

Linear regression is not appropriate here. **Multiclass Logistic Regression** or **Discriminant Analysis** are more appropriate.

---
## Logistic regression

Let’s write `\(p(X) = \Pr(Y = 1 | X)\)` for short and consider using balance to predict default. Logistic regression uses the form

`$$p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}.$$`
( `\(e \approx 2.71828\)` is a mathematical constant, Euler’s number.)

It is easy to see that no matter what values `\(\beta_0\)`, `\(\beta_1\)`, or `\(X\)` take, `\(p(X)\)` will have values between 0 and 1.

--
A bit of rearrangement gives

`$$\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X.$$`
This monotone transformation is called the **log odds** or **logit transformation** of `\(p(X)\)`. (By `\(\log\)` we mean the natural log: `\(\ln\)`.)

---

## Linear versus Logistic Regression

&lt;img src="13_week_p_files/figure-html/unnamed-chunk-5-1.png" width="720" /&gt;

Logistic regression ensures that our estimate for `\(p(X)\)` lies between 0 and 1.
---

## Maximum Likelihood

We use maximum likelihood to estimate the parameters.

`$$\mathcal{l}(\beta_0, \beta_1) = \prod_{i : y_i = 1} p(x_i) \prod_{i : y_i = 0} (1 - p(x_i)).$$`

This likelihood gives the probability of the observed zeros and ones in the data. We pick `\(\beta_0\)` and `\(\beta_1\)` to maximize the likelihood of the observed data.

Most statistical packages can fit linear logistic regression models
by maximum likelihood. In `R` we use the `glm` function


---
## Making prediction 

`$$\begin{array}{c c c c c }
\hline
&amp; \text{Coefficient} &amp; \text{Std. Error} &amp; \text{Z-statistic} &amp; \text{P-value} \\
\hline
\text{Intercept} &amp; -10.6513 &amp; 0.3612 &amp; -29.5 &amp; &lt; 0.0001 \\
\text{Balance} &amp; 0.0055 &amp; 0.0002 &amp; 24.9 &amp; &lt; 0.0001 \\\hline
\end{array}$$`
What is our estimated probability of default for someone with a balance of 1000$?

`$$\hat{p}(X) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 X}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 X}} = \frac{e^{-10.6513 + 0.0055 \times 1000}}{1 + e^{-10.6513 + 0.0055 \times 1000}} = 0.006.$$`
With a balance of 2000$?

--

`$$\hat{p}(X) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 X}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 X}} = \frac{e^{-10.6513 + 0.0055 \times 2000}}{1 + e^{-10.6513 + 0.0055 \times 2000}} = 0.586$$`


---
Lets do it again, using **student** as the predictor.

`$$\begin{array}{c c c c c }
\hline
&amp; \text{Coefficient} &amp; \text{Std. Error} &amp; \text{Z-statistic} &amp; \text{P-value} \\
\hline
\text{Intercept} &amp; -3.5041 &amp; 0.0707 &amp; -49.55 &amp; &lt; 0.0001 \\
\text{student[Yes]} &amp; 0.4049 &amp; 0.1150 &amp; 3.52 &amp; 0.0004 \\
\hline
\end{array}$$`

`$$\Pr(\text{default} = \text{Yes} | \text{student} = \text{Yes}) = \frac{e^{-3.5041 + 0.4049 \times 1}}{1 + e^{-3.5041 + 0.4049 \times 1}} = 0.0431$$`

`$$\Pr(\text{default} = \text{Yes} | \text{student} = \text{No}) = \frac{e^{-3.5041 + 0.4049 \times 0}}{1 + e^{-3.5041 + 0.4049 \times 0}} = 0.0292$$`
---
## Logistic regression with several variables 
`$$\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$`

`$$p(X) = \frac{e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}$$`
`$$\begin{array}{c c c c c }
\hline
&amp; \text{Coefficient} &amp; \text{Std. Error} &amp; \text{Z-statistic} &amp; \text{P-value} \\
\hline
\text{Intercept} &amp; -10.8690 &amp; 0.4923 &amp; -22.08 &amp; &lt; 0.0001 \\
\text{balance} &amp; 0.0057 &amp; 0.0002 &amp; 24.74 &amp; &lt; 0.0001 \\
\text{income} &amp; 0.0030 &amp; 0.0082 &amp; 0.37 &amp; 0.7115 \\
\text{student[Yes]} &amp; -0.6468 &amp; 0.2362 &amp; -2.74 &amp; 0.0062 \\
\hline
\end{array}$$`
Why is coefficient for **student** negative, while it was positive before?
---
## Confounding

&lt;img src="13_week_p_files/figure-html/unnamed-chunk-6-1.png" width="720" /&gt;
 - Students tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.
 - But for each level of balance, students default less than non-students.
 - Multiple logistic regression can tease this out.

---
## Example: South African Heart Disease

- 160 cases of MI (myocardial infarction) and 302 controls (all male in age range 15-64), from Western Cape, South Africa in early 80s.
-  Overall prevalence very high in this region: 5.1%.
- Measurements on seven predictors (risk factors), shown in scatterplot matrix.
- Goal is to identify relative strengths and directions of risk factors.
- This was part of an intervention study aimed at educating the public on healthier diets.


```r
#install.packages('bestglm')
data(SAheart)
```

---

&lt;img src="13_week_p_files/figure-html/unnamed-chunk-8-1.png" width="720" /&gt;
Scatterplot matrix of the South African Heart Disease data. The response is color
coded — The cases (MI) are red, the controls turquoise. `famhist` is a binary variable, with 1 indicating family history of MI.
---


```r
data(SAheart)
heartfit &lt;- glm (chd ~ ., data = SAheart , family = binomial)
summary(heartfit)
```

```
## 
## Call:
## glm(formula = chd ~ ., family = binomial, data = SAheart)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7781  -0.8213  -0.4387   0.8889   2.5435  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    -6.1507209  1.3082600  -4.701 2.58e-06 ***
## sbp             0.0065040  0.0057304   1.135 0.256374    
## tobacco         0.0793764  0.0266028   2.984 0.002847 ** 
## ldl             0.1739239  0.0596617   2.915 0.003555 ** 
## adiposity       0.0185866  0.0292894   0.635 0.525700    
## famhistPresent  0.9253704  0.2278940   4.061 4.90e-05 ***
## typea           0.0395950  0.0123202   3.214 0.001310 ** 
## obesity        -0.0629099  0.0442477  -1.422 0.155095    
## alcohol         0.0001217  0.0044832   0.027 0.978350    
## age             0.0452253  0.0121298   3.728 0.000193 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 596.11  on 461  degrees of freedom
## Residual deviance: 472.14  on 452  degrees of freedom
## AIC: 492.14
## 
## Number of Fisher Scoring iterations: 5
```
---
## Case control 

- In the South African data, there are 160 cases and 302 controls — `\(\tilde{\pi} = 0.35\)` are cases. Yet the prevalence of MI in this region is `\(\pi = 0.05\)`.

- With case-control samples, we can estimate the regression parameters `\(\beta_j\)` accurately (if our model is correct); however, the constant term `\(\beta_0\)` is incorrect.

- We can correct the estimated intercept by a simple transformation:

`$$\hat{\beta}_0^* = \hat{\beta}_0 + \log \frac{\pi}{1 - \pi} - \log \frac{\tilde{\pi}}{1 - \tilde{\pi}}.$$`
---

## Logistic regression with more than two classes

So far we have discussed logistic regression with two classes. It is easily generalized to more than two classes. One version (used in the R package `glmnet`) has the symmetric form:

`$$\Pr(Y = k | X) = \frac{e^{\beta_{0k} + \beta_{1k} X_1 + \dots + \beta_{pk} X_p}}{\sum_{k'=1}^{K} e^{\beta_{0k'} + \beta_{1k'} X_1 + \dots + \beta_{pk'} X_p}}$$`

Here, there is a linear function for **each** class.

Multiclass logistic regression is also referred to as **multinomial regression**.
---
## Discriminant Analysis

- Here, the approach is to model the distribution of `\(X\)` in each of the classes separately, and then use **Bayes' theorem** to flip things around and obtain `\(\Pr(Y | X)\)`.

- When we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis. 

- However, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions.
---
## Bayes theorem for classification

Thomas Bayes was a famous mathematician whose name represents a significant subfield of statistical and probabilistic modeling. Here, we focus on a simple result known as Bayes' theorem:

`$$\Pr(Y = k | X = x) = \frac{\Pr(X = x | Y = k) \cdot \Pr(Y = k)}{\Pr(X = x)}.$$`

One writes this slightly differently for discriminant analysis:

`$$\Pr(Y = k | X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)},$$`

- `\(f_k(x) = \Pr(X = x | Y = k)\)` is the ***density*** for `\(X\)` in class `\(k\)`. Here we will use normal densities for these, separately in each class.
- `\(\pi_k = \Pr(Y = k)\)` is the marginal or **prior** probability for class `\(k\)`.
---
## Classify to the highest density
&lt;img src="13_week_p_files/figure-html/unnamed-chunk-10-1.png" width="720" /&gt;
We classify a new point according to which density is highest.
When the priors are different, we take them into account as well, and compare `\(\pi_k f_k(x)\)`. On the right, we favor the orange class — the decision boundary has shifted to the left.
---
## Why discriminant analysis?

 When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.

If `\(n\)` is small and the distribution of the predictors `\(X\)` is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.

 Linear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data.

---
### Linear Discriminant Analysis when `\(p = 1\)`

The Gaussian density has the form:
$$    f_k(x) = \frac{1}{\sqrt{2\pi\sigma_k}} e^{-\frac{1}{2} \left( \frac{x - \mu_k}{\sigma_k} \right)^2}$$
where `\(\mu_k\)` is the mean, and `\(\sigma_k^2\)` is the variance (in class `\(k\)`). We assume that all `\(\sigma_k = \sigma\)` are the same.

Plugging this into Bayes' formula, we get a rather complex expression for `\(p_k(x) = \Pr(Y = k | X = x)\)`:

`$$p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{1}{2} \left( \frac{x - \mu_k}{\sigma} \right)^2}}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2\pi\sigma}} e^{-\frac{1}{2} \left( \frac{x - \mu_l}{\sigma} \right)^2}}$$`
Happily, there are simplifications and cancellations.

---
### Disciminant function

To classify at the value `\(X = x\)`, we need to see which of the `\(p_k(x)\)` is largest. Taking logs and discarding terms that do not depend on `\(k\)`, we find that this is equivalent to assigning `\(x\)` to the class with the largest **discriminant score**:

`$$\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)$$`

Note that `\(\delta_k(x)\)` is a linear function of `\(x\)`.

If there are `\(K = 2\)` classes and `\(\pi_1 = \pi_2 = 0.5\)`, then one can see that the decision boundary is at:

`$$x = \frac{\mu_1 + \mu_2}{2}$$`
---

```
## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.
## ℹ Please use `after_stat(density)` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.
```

&lt;img src="13_week_p_files/figure-html/unnamed-chunk-11-1.png" width="720" /&gt;
Example with `\(\mu_1=-2, \mu_2 = 2, \sigma^2 = 1\)`

Typically we don’t know these parameters; we just have the training data. In that case we simply estimate the parameters and plug them into the rule.

---
## Estimating the parameters 

`$$\hat{k} = \frac{n_k}{n}$$`
`$$\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i = k} x_i$$`

`$$\hat{\sigma}^2 = \frac{1}{n - K} \sum_{k=1}^{K} \sum_{i: y_i = k} (x_i - \hat{\mu}_k)^2 = \sum_{k=1}^{K} \frac{n_k - 1}{n - K} \cdot \hat{\sigma_k}^2$$`


where 
`$$\hat{\sigma}^2_k = \frac{1}{n_k - 1} \sum_{i: y_i = k} (x_i - \hat{\mu}_k)^2$$`
is the usual formula for the estimated variance in the `\(k\)`th class.

---
## Linear Discriminant Analysis when `\(p &gt; 1\)` 

&lt;center&gt;
&lt;img src="img/multivariate_gauss.png" width="800" height="250" /&gt;
Source: ISLR
&lt;/center&gt;

Density: `\(f(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} e^{-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)}\)`

Discriminant function: `\(\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k\)`

Despite its complex form, `\(\delta_k(x) = c_{k0} + c_{k1} x_1 + c_{k2} x_2 + \dots + c_{kp} x_p\)` — a linear function.

---
### Illustration: p = 2 and K = 3 classes

&lt;center&gt;
&lt;img src="img/decision_boundaries.png" width="800" height="300" /&gt;
Source: ISLR


&lt;/center&gt;

Here `\(\pi_1 = \pi_2 = \pi_3 = 1/3\)`

The dashed lines are known as the Bayes decision boundaries. Were they known, they would yield the fewest misclassification errors, among all possible classifiers.

---
&lt;img src="13_week_p_files/figure-html/unnamed-chunk-12-1.png" width="720" /&gt;

4 variables, 3 species, 50 samples/class.

LDA classifies all but 3 of the 150 training samples correctly

---
&lt;img src="13_week_p_files/figure-html/unnamed-chunk-13-1.png" width="720" /&gt;

---
# From `\(\delta_{k}(x)\)` to probabilities

From `\(\delta_{k}(x)\)` to probabilities

Once we have estimates `\(\delta_{k}(x)\)`, we can turn these into estimates for class probabilities:

`$$\Pr(Y = k \mid X = x) = \frac{e^{\delta_{k}(x)}}{\sum_{l=1}^{K} e^{\delta_{l}(x)}}.$$`

Thus, classifying to the largest `\(\delta_{k}(x)\)` amounts to classifying to the class for which `\(\Pr(Y = k \mid X = x)\)` is largest.

When `\(K = 2\)`, we classify to class 2 if `\(\Pr(Y = 2 \mid X = x) \geq 0.5\)`, 
else to class 1.

---
# LDA on Credit Data

`$$\begin{array}{l|ccc}
\textit{Predicted Default Status} &amp; {\textit{True Default Status}} \\
&amp; \textit{No} &amp; \textit{Yes} &amp; \textbf{Total} \\
\hline
\textit{No} &amp; 9644 &amp; \mathbf{252} &amp; 9896 \\
\textit{Yes} &amp; \mathbf{23} &amp; 81 &amp; 104 \\
\hline
\textbf{Total} &amp; 9667 &amp; \mathbf{333} &amp; 10000
\end{array}$$`

  + This is \textbf{training error}, and we may be overfitting. Not a big concern here since `\(n = 10000\)` and `\(p = 2\)`.
  + If we classified to the prior (i.e., always to class No in this case), we would make `\(\frac{333}{10000} = 3.33\%\)` errors.
  + Of the true No's, we make `\(\frac{23}{9667} \approx 0.2\%\)` errors. 
  + Of the true Yes's, we make `\(\frac{252}{333} \approx 75.7\%\)` errors.


---
## Types of errors 

**False positive rate:** The fraction of negative examples that are classified as positive -- `\(0.2\%\)` in this example.  

**False negative rate:** The fraction of positive examples that are classified as negative -- `\(75.7\%\)` in this example.

We produced this table by classifying to class \textit{Yes} if:
`$$\Pr(\text{Default = Yes} \mid \text{Balance, Student}) \geq 0.5.$$`

We can change the two error rates by changing the threshold from `\(0.5\)` to some other value in `\([0, 1]\)`:
`$$\Pr(\text{Default = Yes} \mid \text{Balance, Student}) \geq \text{threshold};$$`
and vary the threshold.

---
## Other Forms of Discriminant Analysis

`$$\Pr(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}.$$`

When `\(f_k(x)\)` are Gaussian densities with the same covariance matrix `\(\Sigma\)` in each class, this leads to linear discriminant analysis (LDA).

By altering the forms for `\(f_k(x)\)`, we get different classifiers:

  + With Gaussians but different `\(\Sigma_k\)` in each class, we get **quadratic discriminant analysis (QDA)**.
  + With `\(f_k(x) = \prod_{j=1}^p f_{jk}(x_j)\)` (a conditional independence model) in each class, we get **naive Bayes**. For Gaussian densities, this means that `\(\Sigma_k\)` are diagonal.
  + Many other forms arise by proposing specific density models for `\(f_k(x)\)`, including **nonparametric approaches**.

--- 
---
# Naive Bayes

Assumes features are independent in each class.  
Useful when `\(p\)` is large, and so multivariate methods like QDA and even LDA break down.

Gaussian naive Bayes assumes each `\(\Sigma_k\)` is diagonal:  
`$$\delta_k(x) \propto \log \left( \pi_k \prod_{j=1}^p f_{kj}(x_j) \right)
= -\frac{1}{2} \sum_{j=1}^p \left[ \frac{(x_j - \mu_{kj})^2}{\sigma_{kj}^2} + \log \sigma_{kj}^2 \right] + \log \pi_k.$$`

It can be used for **mixed** feature vectors (qualitative and quantitative).  
If `\(X_j\)` is qualitative, replace `\(f_{kj}(x_j)\)` with the probability mass function (histogram) over discrete categories.

Despite strong assumptions, naive Bayes often produces good classification results.

--- 
---
## Logistic Regression versus LDA

For a two-class problem, one can show that for LDA:
`$$\log \frac{p_1(x)}{1 - p_1(x)} = \log \frac{p_1(x)}{p_2(x)} = c_0 + c_1 x_1 + \dots + c_p x_p.$$`

So it has the same form as logistic regression.  
The difference is in how the parameters are estimated.


  + Logistic regression uses the conditional likelihood based on `\(\Pr(Y \mid X)\)` (known as discriminative learning).
  + LDA uses the full likelihood based on `\(\Pr(X, Y)\)` (known as generative learning).

Despite these differences, in practice the results are often very similar.

Footnote: Logistic regression can also fit quadratic boundaries, by explicitly including quadratic terms in the model.

---
---
# Summary
Logistic regression is very popular for classification, especially when `\(K = 2\)`.

LDA is useful when `\(n\)` is small, or the classes are well separated, and Gaussian assumptions are reasonable. It is also useful when `\(K &gt; 2\)`.

Naive Bayes is useful when `\(p\)` is very large.

---
# Sources

- [Introduction to Statistical Learning, Gareth James, Daniela Witten, Trevor Hasie, Robert Tibshirani](https://www.statlearning.com/)
---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atom-one-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
