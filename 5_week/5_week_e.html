<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Applied Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="Tomáš Oleš" />
    <meta name="date" content="2025-01-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Applied Data Analysis
]
.subtitle[
## Analyzing word and document frequency and relationships between words
]
.author[
### Tomáš Oleš
]
.institute[
### Department of Economic Policy
]
.date[
### 2025
]

---








# Motivation (i.)

A central question in **text mining** and **NLP** is:

&gt; How can we quantify *what a document is about*?

---
---

class: middle

# Analyzing word and document frequency: tf-idf

---

## Word importance

- One idea: look at which words appear in a document  
- **Term frequency (tf):** how often a word occurs  
- But: some words appear in every document  
  - “the”, “is”, “of”, etc.  
- These are not informative!

---

## Beyond stop words

We can remove common words (stop words),  
but this is a crude approach.

&gt; We need a dynamic way to **down-weight ubiquitous words**.

---

# Inverse document frequency (idf)

Another idea: give less weight to words  
that appear in many documents.

`$$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$`

---

## Combining tf and idf

The **tf–idf** score = term frequency × inverse document frequency

- High tf–idf → frequent in this document, rare elsewhere  
- Low tf–idf → common word shared across documents  

---

## Interpretation

&gt; tf–idf measures how important a word is to a document  
&gt; within a collection of documents (the *corpus*).

It’s a **heuristic**—useful in search engines, topic modeling, and NLP.

---

# Example: Jane Austen’s novels

We’ll start with the published novels of **Jane Austen**  
and examine term frequency, then tf–idf.

---

## Data setup


```r
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words &lt;- austen_books() %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(book, word, sort = TRUE)

total_words &lt;- book_words %&gt;% 
  group_by(book) %&gt;% 
  summarize(total = sum(n))

book_words &lt;- left_join(book_words, total_words)
```

```
## Joining with `by = join_by(book)`
```

```r
book_words
```

```
## # A tibble: 40,379 × 4
##   book           word      n  total
##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;int&gt;
## 1 Mansfield Park the    6206 160460
## 2 Mansfield Park to     5475 160460
## 3 Mansfield Park and    5438 160460
## # ℹ 40,376 more rows
```
---
### Term frequency distribution


```r
library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-3-1.png" width="38%" style="display: block; margin: auto;" /&gt;

---
### Zipf’s Law

 &gt; The frequency of a word is inversely proportional to its rank.

 This relationship is common in natural language corpora.

```r
freq_by_rank &lt;- book_words %&gt;%
  group_by(book) %&gt;%
  mutate(rank = row_number(),
         term_frequency = n/total)
freq_by_rank
```

```
## # A tibble: 40,379 × 6
## # Groups:   book [6]
##   book           word      n  total  rank term_frequency
##   &lt;fct&gt;          &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;
## 1 Mansfield Park the    6206 160460     1         0.0387
## 2 Mansfield Park to     5475 160460     2         0.0341
## 3 Mansfield Park and    5438 160460     3         0.0339
## # ℹ 40,376 more rows
```
---
### Zipf's law


```r
freq_by_rank %&gt;% 
  ggplot(aes(rank, term_frequency, color = book)) + 
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-5-1.png" width="50%" style="display: block; margin: auto;" /&gt;



---
### Term frequency distribution


```r
rank_subset &lt;- freq_by_rank %&gt;%
  filter(rank &lt; 500, rank &gt; 10)

lm(log10(term_frequency) ~ log10(rank), data = rank_subset)
```

```
## 
## Call:
## lm(formula = log10(term_frequency) ~ log10(rank), data = rank_subset)
## 
## Coefficients:
## (Intercept)  log10(rank)  
##     -0.6226      -1.1125
```


---
### Visualizing with the fitted line


```r
freq_by_rank %&gt;% 
  ggplot(aes(rank, term_frequency, color = book)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray50", linetype = 2) +
  geom_line(linewidth = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-7-1.png" width="40%" style="display: block; margin: auto;" /&gt;

---

### Computing the tf-idf

Use `bind_tf_idf()` from `tidytext`.


```r
book_tf_idf &lt;- book_words %&gt;%
  bind_tf_idf(word, book, n)

# Inspect the tf-idf
book_tf_idf %&gt;%
  arrange(desc(tf_idf))
```

```
## # A tibble: 40,379 × 7
##   book                word         n  total      tf   idf  tf_idf
##   &lt;fct&gt;               &lt;chr&gt;    &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 Sense &amp; Sensibility elinor     623 119957 0.00519  1.79 0.00931
## 2 Sense &amp; Sensibility marianne   492 119957 0.00410  1.79 0.00735
## 3 Mansfield Park      crawford   493 160460 0.00307  1.79 0.00551
## # ℹ 40,376 more rows
```

---


```r
book_tf_idf %&gt;%
  group_by(book) %&gt;%
  slice_max(tf_idf, n = 15) %&gt;%
  ungroup() %&gt;%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-9-1.png" width="50%" style="display: block; margin: auto;" /&gt;
---

### Example: classic physics texts 


Let’s try the same on scientific works:

 + Galileo: Discourse on Floating Bodies

 + Huygens: Treatise on Light

 + Tesla: Experiments with Alternate Currents

 + Einstein: Relativity: The Special and General Theory


```r
library(gutenbergr)

physics &lt;- gutenberg_download(
  c(37729, 14725, 13476, 30155),
  meta_fields = "author"
)

physics_words &lt;- physics %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(author, word, sort = TRUE)
```

---


```r
physics_words
```

```
## # A tibble: 12,668 × 3
##   author              word      n
##   &lt;chr&gt;               &lt;chr&gt; &lt;int&gt;
## 1 Galilei, Galileo    the    3760
## 2 Tesla, Nikola       the    3604
## 3 Huygens, Christiaan the    3553
## # ℹ 12,665 more rows
```


```r
plot_physics &lt;- physics_words %&gt;%
  bind_tf_idf(word, author, n) %&gt;%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))
```

---


```r
plot_physics %&gt;% 
  group_by(author) %&gt;% 
  slice_max(tf_idf, n = 15) %&gt;% 
  ungroup() %&gt;%
  mutate(word = reorder(word, tf_idf)) %&gt;%
  ggplot(aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~author, ncol = 2, scales = "free")
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-13-1.png" width="40%" style="display: block; margin: auto;" /&gt;
---
--- 


```r
library(stringr)

physics %&gt;% 
  filter(str_detect(text, "_k_")) %&gt;% 
  select(text)
```

```
## # A tibble: 7 × 1
##   text                                                                
##   &lt;chr&gt;                                                               
## 1 surface AB at the points AK_k_B. Then instead of the hemispherical  
## 2 would needs be that from all the other points K_k_B there should    
## 3 necessarily be equal to CD, because C_k_ is equal to CK, and C_g_ to
## # ℹ 4 more rows
```

---
--- 
### Custom stop words removal

```r
mystopwords &lt;- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

physics_words &lt;- anti_join(physics_words, mystopwords, 
                           by = "word")
```

---
--- 


```r
plot_physics &lt;- physics_words %&gt;%
  bind_tf_idf(word, author, n) %&gt;%
  mutate(word = str_remove_all(word, "_")) %&gt;%
  group_by(author) %&gt;% 
  slice_max(tf_idf, n = 15) %&gt;%
  ungroup() %&gt;%
  mutate(word = fct_reorder(word, tf_idf)) %&gt;%
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))

ggplot(plot_physics, aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-16-1.png" width="30%" style="display: block; margin: auto;" /&gt;
---
### Usecase in economics

- **Applications:**
  - **Labor economics:** Identify skill content in job vacancies or resumes (e.g., Deming &amp; Kahn, 2018; Modestino et al., 2020).  
    → TF–IDF weights reflect demand intensity for each skill.
  - **Innovation and productivity:** Measure technological similarity between patents and firms’ text disclosures.  
    → Used to link innovation clusters and productivity spillovers.
  - **Political economy &amp; finance:** Extract themes from speeches, central bank statements, or reports.  
    → TF–IDF helps detect attention shifts or sentiment changes over time.

- **Advantages:**
  - Simple, interpretable weighting scheme  
  - Reduces dominance of frequent but uninformative words  
  - Useful as input for clustering, topic modeling, or regression analysis
  
&gt; After learning some regression techniques based on TF-IDF, we will classify emails as spam or ham (13th week). This can easily be extended to automation and non-automation innovations, as pioneered by Mann &amp; Putman (2023).
---

class: middle

# Relationships between words: n-grams and correlations

---
--- 
# Motivation (ii.)

So far we treated **words as individual units**.

Now we analyze **relationships between words**:
- Which words **follow** others
- Which words **co-occur** in the same documents/sections

---
--- 

## Toolbox

We’ll use:
- `unnest_tokens(..., token = "ngrams")` for bigrams/trigrams  
- **ggraph** for network plots  
- **widyr** for pairwise counts &amp; correlations

---
## Tokenizing by n-gram

Tokenize into consecutive sequences of words (**n-grams**).


```r
library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams &lt;- austen_books() %&gt;%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %&gt;%
  filter(!is.na(bigram))

head(austen_bigrams, 5)
```

```
## # A tibble: 5 × 2
##   book                bigram         
##   &lt;fct&gt;               &lt;chr&gt;          
## 1 Sense &amp; Sensibility sense and      
## 2 Sense &amp; Sensibility and sensibility
## 3 Sense &amp; Sensibility by jane        
## # ℹ 2 more rows
```

&gt; Still tidy: one token per row, with metadata (e.g., book) preserved.

---
### Count the most common n-gram



```r
austen_bigrams %&gt;%
  count(bigram, sort = TRUE)
```

```
## # A tibble: 193,209 × 2
##   bigram     n
##   &lt;chr&gt;  &lt;int&gt;
## 1 of the  2853
## 2 to be   2670
## 3 in the  2221
## # ℹ 193,206 more rows
```

```r
head(austen_bigrams, 5)
```

```
## # A tibble: 5 × 2
##   book                bigram         
##   &lt;fct&gt;               &lt;chr&gt;          
## 1 Sense &amp; Sensibility sense and      
## 2 Sense &amp; Sensibility and sensibility
## 3 Sense &amp; Sensibility by jane        
## # ℹ 2 more rows
```

---

Split into words, remove stop words


```r
library(tidyr)

bigrams_separated &lt;- austen_bigrams %&gt;%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word)

bigram_counts &lt;- bigrams_filtered %&gt;%
  count(word1, word2, sort = TRUE)

head(bigram_counts, 1)
```

```
## # A tibble: 1 × 3
##   word1 word2      n
##   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;
## 1 sir   thomas   266
```

```r
bigrams_united &lt;- bigrams_filtered %&gt;%
  unite(bigram, word1, word2, sep = " ")

head(bigrams_united, 1)
```

```
## # A tibble: 1 × 2
##   book                bigram     
##   &lt;fct&gt;               &lt;chr&gt;      
## 1 Sense &amp; Sensibility jane austen
```

---

### Trigrams **n=3**.


```r
austen_books() %&gt;%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %&gt;%
  filter(!is.na(trigram)) %&gt;%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %&gt;%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %&gt;%
  count(word1, word2, word3, sort = TRUE)
```

```
## # A tibble: 6,140 × 4
##   word1 word2     word3         n
##   &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;
## 1 dear  miss      woodhouse    20
## 2 miss  de        bourgh       17
## 3 lady  catherine de           11
## # ℹ 6,137 more rows
```

---


### Analyzing bigrams


```r
bigrams_filtered %&gt;%
  filter(word2 == "street") %&gt;%
  count(book, word1, sort = TRUE)
```

```
## # A tibble: 33 × 3
##   book                word1        n
##   &lt;fct&gt;               &lt;chr&gt;    &lt;int&gt;
## 1 Sense &amp; Sensibility harley      16
## 2 Sense &amp; Sensibility berkeley    15
## 3 Northanger Abbey    milsom      10
## # ℹ 30 more rows
```

---
### TF-IDF on bigrams 


```r
bigram_tf_idf &lt;- bigrams_united %&gt;%
  count(book, bigram) %&gt;%
  bind_tf_idf(bigram, book, n) %&gt;%
  arrange(desc(tf_idf))

bigram_tf_idf
```

```
## # A tibble: 31,391 × 6
##   book           bigram                n     tf   idf tf_idf
##   &lt;fct&gt;          &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 Mansfield Park sir thomas          266 0.0304  1.79 0.0545
## 2 Persuasion     captain wentworth   143 0.0290  1.79 0.0519
## 3 Mansfield Park miss crawford       196 0.0224  1.79 0.0402
## # ℹ 31,388 more rows
```

---

### Bigrams by book 


```r
bigram_tf_idf %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  group_by(book) %&gt;%
  slice_max(tf_idf, n = 10) %&gt;%
  ungroup() %&gt;%
  mutate(bigram = reorder(bigram, tf_idf)) %&gt;%
  ggplot(aes(tf_idf, bigram, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ book, ncol = 2, scales = "free") +
  labs(x = "tf–idf of bigram", y = NULL)
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-23-1.png" width="40%" style="display: block; margin: auto;" /&gt;

&gt; Using bigrams for sentiment context. Problem with simple lexicon counts: ignores negation. Example: “not happy” should reverse sentiment.

---

### Find 'not X' bigrams 


```r
bigrams_separated %&gt;%
  filter(word1 == "not") %&gt;%
  count(word1, word2, sort = TRUE)
```

```
## # A tibble: 1,178 × 3
##   word1 word2     n
##   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;
## 1 not   be      580
## 2 not   to      335
## 3 not   have    307
## # ℹ 1,175 more rows
```


```r
#Join with AFINN scores 
AFINN &lt;- get_sentiments("afinn")

not_words &lt;- bigrams_separated %&gt;%
  filter(word1 == "not") %&gt;%
  inner_join(AFINN, by = c(word2 = "word")) %&gt;%
  count(word2, value, sort = TRUE)

head(not_words, 3)
```

```
## # A tibble: 3 × 3
##   word2 value     n
##   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;
## 1 like      2    95
## 2 help      2    77
## 3 want      1    41
```

---

### Which 'not X' drives misclassification?


```r
not_words %&gt;%
  mutate(contribution = n * value) %&gt;%
  arrange(desc(abs(contribution))) %&gt;%
  head(20) %&gt;%
  mutate(word2 = reorder(word2, contribution)) %&gt;%
  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Sentiment value × occurrences",
       y = "Words preceded by \"not\"")
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-26-1.png" width="40%" style="display: block; margin: auto;" /&gt;
---
--- 

### Broaden negations 


```r
negation_words &lt;- c("not", "no", "never", "without")

negated_words &lt;- bigrams_separated %&gt;%
  filter(word1 %in% negation_words) %&gt;%
  inner_join(AFINN, by = c(word2 = "word")) %&gt;%
  count(word1, word2, value, sort = TRUE)
```

---

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-28-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

### Bigrams with networks with `ggraph`

We can visualize all relationships (not just top few) as a graph.

- from: word1

- to: word2

- weight: count n


```r
library(igraph)

bigram_graph &lt;- bigram_counts %&gt;%
  filter(n &gt; 20) %&gt;%        # keep common bigrams
  graph_from_data_frame()

bigram_graph
```

```
## IGRAPH 98162a7 DN-- 85 70 -- 
## + attr: name (v/c), n (e/n)
## + edges from 98162a7 (vertex names):
##  [1] sir     -&gt;thomas     miss    -&gt;crawford   captain -&gt;wentworth 
##  [4] miss    -&gt;woodhouse  frank   -&gt;churchill  lady    -&gt;russell   
##  [7] sir     -&gt;walter     lady    -&gt;bertram    miss    -&gt;fairfax   
## [10] colonel -&gt;brandon    sir     -&gt;john       miss    -&gt;bates     
## [13] jane    -&gt;fairfax    lady    -&gt;catherine  lady    -&gt;middleton 
## [16] miss    -&gt;tilney     miss    -&gt;bingley    thousand-&gt;pounds    
## [19] miss    -&gt;dashwood   dear    -&gt;miss       miss    -&gt;bennet    
## [22] miss    -&gt;morland    captain -&gt;benwick    miss    -&gt;smith     
## + ... omitted several edges
```

---



```r
library(ggraph)
set.seed(42)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-30-1.png" width="40%" style="display: block; margin: auto;" /&gt;


---

```r
set.seed(42)
a &lt;- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-31-1.png" width="40%" style="display: block; margin: auto;" /&gt;
---
### Tokenize the books

```r
library(tidytext)

austen_section_words &lt;- austen_books() %&gt;%
  filter(book == "Pride &amp; Prejudice") %&gt;%
  mutate(section = row_number() %/% 10) %&gt;%
  filter(section &gt; 0) %&gt;%
  unnest_tokens(word, text) %&gt;%
  filter(!word %in% stop_words$word)

head(austen_section_words, 3)
```

```
## # A tibble: 3 × 3
##   book              section word        
##   &lt;fct&gt;               &lt;dbl&gt; &lt;chr&gt;       
## 1 Pride &amp; Prejudice       1 truth       
## 2 Pride &amp; Prejudice       1 universally 
## 3 Pride &amp; Prejudice       1 acknowledged
```
---
### Motivation (iii.)



```r
library(widyr)

# count words co-occuring within sections
word_pairs &lt;- austen_section_words %&gt;%
  pairwise_count(word, section, sort = TRUE)

head(word_pairs, 3)
```

```
## # A tibble: 3 × 3
##   item1     item2         n
##   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;
## 1 darcy     elizabeth   144
## 2 elizabeth darcy       144
## 3 miss      elizabeth   110
```

Pairs like **“Elizabeth”** and **“Darcy”** are the most common co-occurring words, but that’s not particularly meaningful — they’re also the most common individual words.

We want to examine **correlation among words**,  which indicates how often they appear together  relative to how often they appear separately.

--- 
---

### Binary correlation

The **phi coefficient** ($\phi$) is a common measure for binary correlation.

It measures how much more likely it is that  both word X and word Y appear (or neither do)  than that one appears without the other.

&gt; Contingency table

|                     | Has word Y | No word Y | Total |
|:--------------------|:-----------:|:-----------:|:------:|
| **Has word X**      | `\(n_{11}\)` | `\(n_{10}\)` | `\(n_{1\cdot}\)` |
| **No word X**       | `\(n_{01}\)` | `\(n_{00}\)` | `\(n_{0\cdot}\)` |
| **Total**           | `\(n_{\cdot1}\)` | `\(n_{\cdot0}\)` | `\(n\)` |


&gt; Interpreting the counts

- `\(n_{11}\)`: number of documents where **both X and Y** appear  
- `\(n_{00}\)`: number of documents where **neither** appears  
- `\(n_{10}\)`, `\(n_{01}\)`: cases where **only one** appears  

---

### Formula for binary (pairwise) correlation `\(\phi\)`

`$$\phi = \frac{n_{11}n_{00} - n_{10}n_{01}} {\sqrt{\,n_{1\cdot} n_{0\cdot} n_{\cdot0} n_{\cdot1}\,}}$$`

The phi coefficient is **equivalent to the Pearson correlation**  
when applied to **binary data**.

---

### Compute pairwise correlations in R



```r
library(widyr)
word_cors &lt;- austen_section_words %&gt;%
  group_by(word) %&gt;%
  filter(n() &gt;= 20) %&gt;%            # focus on common words
  pairwise_cor(word, section, sort = TRUE)

word_cors
```

```
## # A tibble: 154,842 × 3
##   item1  item2    correlation
##   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;
## 1 bourgh de             0.951
## 2 de     bourgh         0.951
## 3 pounds thousand       0.701
## # ℹ 154,839 more rows
```


```r
word_cors %&gt;%
  filter(item1 == "pounds")
```

```
## # A tibble: 393 × 3
##   item1  item2    correlation
##   &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;
## 1 pounds thousand       0.701
## 2 pounds ten            0.231
## 3 pounds fortune        0.164
## # ℹ 390 more rows
```


---


```r
word_cors %&gt;%
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride")) %&gt;%
  group_by(item1) %&gt;%
  slice_max(correlation, n = 6) %&gt;%
  ungroup() %&gt;%
  mutate(item2 = reorder(item2, correlation)) %&gt;%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  labs(x = NULL, y = "Correlation (ϕ)")
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-36-1.png" width="40%" style="display: block; margin: auto;" /&gt;

---


```r
set.seed(42)

word_cors %&gt;%
  filter(correlation &gt; .15) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

&lt;img src="5_week_e_files/figure-html/unnamed-chunk-37-1.png" width="40%" style="display: block; margin: auto;" /&gt;


---
# Sources
- [Silge, J., Robinson D. (2025). Text Mining with R!. O'Reilly](https://www.tidytextmining.com/)

---

## References

- [Deming, D. J., &amp; Kahn, L. B. (2018). *Skill Requirements across Firms and Labor Markets: Evidence from Job Postings.* The Journal of Labor Economics.](https://doi.org/10.1086/694106)

- [Modestino, A. S., Shoag, D., &amp; Ballance, J. (2020). *Downskilling: Changes in Employer Skill Requirements over the Business Cycle.* Labour Economics, 65, 101821.](https://doi.org/10.1016/j.labeco.2020.101821)

- [Mann, K., &amp; Putman, L. (2023). *Benign Effects of Automation.* Working Paper, CBS.](https://research-api.cbs.dk/ws/portalfiles/portal/71105288/katja_mann_et_al_benign_effects_of_automation_acceptedversion.pdf)

- [Silge, J., &amp; Robinson, D. (2025). *Text Mining with R!* O’Reilly.](https://www.tidytextmining.com/)



---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atom-one-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
