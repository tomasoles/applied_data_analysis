<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Applied Data Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="TomÃ¡Å¡ OleÅ¡" />
    <meta name="date" content="2025-01-01" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Applied Data Analysis
]
.subtitle[
## Text tidying and sentiment analysis
]
.author[
### TomÃ¡Å¡ OleÅ¡
]
.institute[
### Department of Economic Policy
]
.date[
### 2025
]

---








---


class: middle

# The tidy text format

---

# Text as a data 

* **String**: Text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.

* **Corpus**: These types of objects typically contain raw strings annotated with additional metadata and details.

* **Document-term matrix**: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf.
---


## Character vectors


```r
text &lt;- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text
```

```
## [1] "Because I could not stop for Death -"  
## [2] "He kindly stopped for me -"            
## [3] "The Carriage held but just Ourselves -"
## [4] "and Immortality"
```
We can turn this into tidy dataset


```r
library(dplyr)
text_df &lt;- tibble(line = 1:4, text = text)
```

---

### Tokenization 

Tokenization - A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis.



```r
library(tidytext)

text_df %&gt;%
  unnest_tokens(word, text)
```

```
## # A tibble: 20 Ã— 2
##    line word   
##   &lt;int&gt; &lt;chr&gt;  
## 1     1 because
## 2     1 i      
## 3     1 could  
## 4     1 not    
## 5     1 stop   
## # â„¹ 15 more rows
```
---
### Tokenization `unnest_tokens()`

+ Split each row so that there is one token (word) in each row of the new data frame
+ Punctuation has been removed 
+ Turned into lower case (`to_lower = TRUE`)

The default tokenizing is for words, but other options include **characters, n-grams, sentences, lines, paragraphs, or separation around a regex pattern**. (... we will get use to it)



&lt;div class="figure"&gt;
&lt;img src="img/tmwr_0101.png" alt="A flowchart of a typical text analysis using tidy data principles." width="100%" /&gt;
&lt;p class="caption"&gt;A flowchart of a typical text analysis using tidy data principles.&lt;/p&gt;
&lt;/div&gt;
--- 
---
## Tidying the text


```r
library(janeaustenr)
library(dplyr)
library(stringr)

original_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %&gt;%
  ungroup()

#original_books
```
Tokenize 


```r
tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text)

#tidy_books
```
What is the dimension of `original_books` and `tidy_books`?
---
--- 
### Let's plot the distribution of tokens 
&lt;img src="5_week_p_files/figure-html/unnamed-chunk-5-1.png" width="504" /&gt;
.question[*Something unusual?*]
---

### Clean the stop-words 


```r
data(stop_words)

tidy_books &lt;- tidy_books %&gt;%
  anti_join(stop_words, by = join_by(word))

tidy_books %&gt;%
  count(word, sort = TRUE) 
```

```
## # A tibble: 13,914 Ã— 2
##   word      n
##   &lt;chr&gt; &lt;int&gt;
## 1 miss   1855
## 2 time   1337
## 3 fanny   862
## 4 dear    822
## 5 lady    817
## # â„¹ 13,909 more rows
```

What are the Austen's books about? ðŸ‘©

--- 
---

## Word frequencies


```r
# Jane Eyre, Wuthering Heights
library(gutenbergr)

hgwells &lt;- gutenberg_download(c(35, 36))

tidy_hgwells &lt;- hgwells %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)
```

```
## Joining with `by = join_by(word)`
```
---


```r
bronte &lt;- gutenberg_download(c(1260, 768))

tidy_bronte &lt;- bronte %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)
```

```
## Joining with `by = join_by(word)`
```

```r
tidy_bronte %&gt;%
  count(word, sort = TRUE)
```

```
## # A tibble: 15,328 Ã— 2
##   word           n
##   &lt;chr&gt;      &lt;int&gt;
## 1 miss         439
## 2 heathcliff   421
## 3 time         372
## 4 sir          359
## 5 linton       346
## # â„¹ 15,323 more rows
```

---


```r
library(tidyr)

frequency &lt;- bind_rows(mutate(tidy_bronte, author = "BrontÃ« Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %&gt;% 
  mutate(word = str_extract(word, "[a-z']+")) %&gt;%
  count(author, word) %&gt;%
  group_by(author) %&gt;%
  mutate(proportion = n / sum(n)) %&gt;% 
  select(-n) %&gt;% 
  pivot_wider(names_from = author, values_from = proportion) %&gt;%
  pivot_longer(`BrontÃ« Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

frequency
```

```
## # A tibble: 44,760 Ã— 4
##   word    `Jane Austen` author          proportion
##   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;
## 1 a          0.00000919 BrontÃ« Sisters  0.000121  
## 2 a          0.00000919 H.G. Wells      0.0000293 
## 3 abaht     NA          BrontÃ« Sisters  0.00000934
## 4 abaht     NA          H.G. Wells     NA         
## 5 abandon   NA          BrontÃ« Sisters  0.0000374 
## # â„¹ 44,755 more rows
```

---


```r
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```

---


&lt;img src="5_week_p_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

class: middle

# Sentiment analysis with tidy data

---



### Using **tidytext**, **dplyr**, and friends in R



&lt;div class="figure"&gt;
&lt;img src="img/tmwr_0201.png" alt="A flowchart of a typical text analysis that uses tidytext for sentiment analysis." width="100%" /&gt;
&lt;p class="caption"&gt;A flowchart of a typical text analysis that uses tidytext for sentiment analysis.&lt;/p&gt;
&lt;/div&gt;


This deck covers: tidy sentiment lexicons, joining lexicons to tokenized text, computing sentiment trajectories, comparing dictionaries, most-influential words, wordclouds, and sentence/chapter tokenization.


---

## What is sentiment analysis?


+ Programmatic approach to estimate **opinion/emotion** in text (positive/negative or emotions like joy, anger).
+ Tidy approach: treat text as **tokens (words)**; compute document sentiment by **combining word-level scores**.
+ Caveats: negations (e.g., *"not good"*), sarcasm, domain mismatch, and chunk size effects.


---

## Packages

```r
#install.packages(c("tidytext","janeaustenr","dplyr","stringr","tidyr","ggplot2","wordcloud","reshape2"))

library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(reshape2)
```

---

## Sentiment lexicons available via `tidytext`



+ **AFINN** (âˆ’5â€¦+5 numeric scores)
+ **bing** (binary: positive/negative)
+ **nrc** (binary: pos/neg + emotions: joy, anger, fear, trust, sadness, surprise, disgust, anticipation)

Get any lexicon:



```r
library(tidytext)
library(textdata)
library(dplyr)
get_sentiments("afinn")   # or "bing", "nrc"
```

```
## # A tibble: 2,477 Ã— 2
##   word      value
##   &lt;chr&gt;     &lt;dbl&gt;
## 1 abandon      -2
## 2 abandoned    -2
## 3 abandons     -2
## 4 abducted     -2
## 5 abduction    -2
## # â„¹ 2,472 more rows
```


**Licensing matters**: check each lexiconâ€™s license before use.


---

## Build a tidy corpus from Jane Austen


```r
# 1 token per row, with book/line/chapter metadata

tidy_books &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))
  ) %&gt;%
  ungroup() %&gt;%
  unnest_tokens(word, text)
```


We name the token column **word** to match lexicons, making joins simpler.


---

## Joy words in *Emma* (NRC)


```r
nrc_joy &lt;- get_sentiments("nrc") %&gt;% filter(sentiment == "joy")

joy_counts &lt;- tidy_books %&gt;%
  filter(book == "Emma") %&gt;%
  inner_join(nrc_joy, by = "word") %&gt;%
  count(word, sort = TRUE)

head(joy_counts, 15)
```

```
## # A tibble: 15 Ã— 2
##   word       n
##   &lt;chr&gt;  &lt;int&gt;
## 1 good     359
## 2 friend   166
## 3 hope     143
## 4 happy    125
## 5 love     117
## # â„¹ 10 more rows
```

---

## Compute sentiment through each novel (Bing)


```r
jane_austen_sentiment &lt;- tidy_books %&gt;%
  inner_join(get_sentiments("bing"), by = "word") %&gt;%
  count(book, index = linenumber %/% 80, sentiment) %&gt;%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%
  mutate(sentiment = positive - negative)
```

Tip: `%/%` does **integer division**; choose bin size to balance noise vs smoothing.

---

### Visualize sentiment through each novel (Bing)


```r
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") +
  labs(x = "Narrative index (80-line bins)", y = "Net sentiment (pos - neg)")
```

&lt;img src="5_week_p_files/figure-html/unnamed-chunk-16-1.png" width="50%" style="display: block; margin: auto;" /&gt;
---

## Compare AFINN vs Bing vs NRC on *Pride and Prejudice*


```r
pp &lt;- tidy_books %&gt;% filter(book == "Pride &amp; Prejudice")

afinn &lt;- pp %&gt;%
  inner_join(get_sentiments("afinn"), by = "word") %&gt;%
  group_by(index = linenumber %/% 80) %&gt;%
  summarise(sentiment = sum(value), .groups = "drop") %&gt;%
  mutate(method = "AFINN")

bing_nrc &lt;- bind_rows(
  pp %&gt;% inner_join(get_sentiments("bing"), by = "word") %&gt;% mutate(method = "Bing"),
  pp %&gt;% inner_join(get_sentiments("nrc") %&gt;% filter(sentiment %in% c("positive","negative")), by = "word") %&gt;% mutate(method = "NRC")
) %&gt;%
  count(method, index = linenumber %/% 80, sentiment) %&gt;%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;%
  mutate(sentiment = positive - negative)
```
---
### Visualize across sentiment lexicons


```r
bind_rows(afinn, bing_nrc) %&gt;%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  labs(x = "Narrative index", y = "Net sentiment")
```

&lt;img src="5_week_p_files/figure-html/unnamed-chunk-18-1.png" width="50%" style="display: block; margin: auto;" /&gt;

**Patterns**: similar ups/downs across methods; different absolute scales (AFINN often larger; NRC skewed more positive; Bing shows longer runs).


---

## Why NRC skews higher?


```r
get_sentiments("nrc") %&gt;%
  filter(sentiment %in% c("positive","negative")) %&gt;%
  count(sentiment)
```

```
## # A tibble: 2 Ã— 2
##   sentiment     n
##   &lt;chr&gt;     &lt;int&gt;
## 1 negative   3316
## 2 positive   2308
```

```r
get_sentiments("bing") %&gt;%
  count(sentiment)
```

```
## # A tibble: 2 Ã— 2
##   sentiment     n
##   &lt;chr&gt;     &lt;int&gt;
## 1 negative   4781
## 2 positive   2005
```


Lexicon composition differs (ratio of negative to positive tokens). Also, word-match coverage vs author vocabulary matters.


---

## Most influential words (Bing)


```r
bing_word_counts &lt;- tidy_books %&gt;%
  inner_join(get_sentiments("bing"), by = "word") %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  ungroup()

bing_word_counts
```

```
## # A tibble: 2,585 Ã— 3
##   word  sentiment     n
##   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
## 1 miss  negative   1855
## 2 well  positive   1523
## 3 good  positive   1380
## 4 great positive    981
## 5 like  positive    725
## # â„¹ 2,580 more rows
```

---


```r
bing_word_counts %&gt;%
  group_by(sentiment) %&gt;%
  slice_max(n, n = 10) %&gt;%
  ungroup() %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment", y = NULL)
```

&lt;img src="5_week_p_files/figure-html/unnamed-chunk-21-1.png" width="40%" style="display: block; margin: auto;" /&gt;


Watch for anomalies like **"miss"** (title vs negative verb); consider a **custom stop-word list**.


---

## Custom stop words example


```r
custom_stop_words &lt;- bind_rows(
  tibble(word = c("miss"), lexicon = c("custom")),
  stop_words
)
```

---

## Wordclouds


```r
# All words, excluding stop words
library(wordcloud)
tidy_books %&gt;%
  anti_join(stop_words, by = "word") %&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100))
```

&lt;img src="5_week_p_files/figure-html/unnamed-chunk-23-1.png" width="50%" style="display: block; margin: auto;" /&gt;
---

```r
# Comparison cloud: positive vs negative (Bing)
library(reshape2)
tidy_books %&gt;%
  inner_join(get_sentiments("bing"), by = "word") %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = "n", fill = 0) %&gt;%
  comparison.cloud(colors = c("gray20","gray80"), max.words = 100)
```

&lt;img src="5_week_p_files/figure-html/unnamed-chunk-24-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---

## Beyond words: sentences &amp; chapters


```r
# Sentence tokenization (e.g., for negation-aware methods)
pp_sentences &lt;- tibble(text = prideprejudice) %&gt;%
  unnest_tokens(sentence, text, token = "sentences")
pp_sentences$sentence[2]
```

```
## [1] "by jane austen"
```


```r
# Chapter tokenization via regex

austen_chapters &lt;- austen_books() %&gt;%
  group_by(book) %&gt;%
  unnest_tokens(chapter, text, token = "regex",
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %&gt;%
  ungroup()
```
---


```r
austen_chapters %&gt;% group_by(book) %&gt;% summarise(chapters = n())
```

```
## # A tibble: 6 Ã— 2
##   book                chapters
##   &lt;fct&gt;                  &lt;int&gt;
## 1 Sense &amp; Sensibility       51
## 2 Pride &amp; Prejudice         62
## 3 Mansfield Park            49
## 4 Emma                      56
## 5 Northanger Abbey          32
## # â„¹ 1 more row
```

---

### Most negative chapter per book (normalized)


```r
bing_negative &lt;- get_sentiments("bing") %&gt;% filter(sentiment == "negative")

wordcounts &lt;- tidy_books %&gt;%
  group_by(book, chapter) %&gt;%
  summarise(words = n(), .groups = "drop")

neg_chapters &lt;- tidy_books %&gt;%
  semi_join(bing_negative, by = "word") %&gt;%
  group_by(book, chapter) %&gt;%
  summarise(negativewords = n(), .groups = "drop") %&gt;%
  left_join(wordcounts, by = c("book","chapter")) %&gt;%
  mutate(ratio = negativewords / words) %&gt;%
  filter(chapter != 0) %&gt;%
  arrange(book)
```
---

```r
most_neg_chapters &lt;- neg_chapters %&gt;%
  group_by(book) %&gt;%
  slice_max(ratio, n = 1) 

most_neg_chapters
```

```
## # A tibble: 6 Ã— 5
## # Groups:   book [6]
##   book                chapter negativewords words  ratio
##   &lt;fct&gt;                 &lt;int&gt;         &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
## 1 Sense &amp; Sensibility      43           161  3405 0.0473
## 2 Pride &amp; Prejudice        34           111  2104 0.0528
## 3 Mansfield Park           46           173  3685 0.0469
## 4 Emma                     15           151  3340 0.0452
## 5 Northanger Abbey         21           149  2982 0.0500
## # â„¹ 1 more row
```
---

## Practical caveats &amp; tips



* **Negation/sarcasm**: unigram lexicons can miss scope; consider sentence-level or dependency-based approaches.
* **Domain mismatch**: validate lexicon choice on your domain; try multiple lexicons and compare.
* **Chunk size**: tune bin size (lines, sentences, paragraphs) to retain narrative structure without excessive noise.
* **Preprocessing**: be consistent (lowercasing, tokenization, handling of names/characters).
* **Reproducibility**: set seeds where randomness appears; record package versions.


---
# Sources
- [Silge, J., Robinson D. (2025). Text Mining with R!. O'Reilly](https://www.tidytextmining.com/)

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "atom-one-light",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
